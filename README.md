# LLM_Finetuning (Large language model_Fine tuning)
## Introduction
大型語言模型（Large Language Model，簡稱LLM）是一種類似於GPT-4這樣的人工智能模型，專門設計來處理和生成自然語言，而這些模型基於大量的文本數據進行訓練，進一步學習人類的語言結構和語法以及語義，以能夠理解和生成與人類語言類似的文字內容。
大型語言模型的特點：
1. 規模大：模型通常包含數十億甚至數千億個參數（類似於大腦中的神經元），使它們能夠有效地處理大規模的語言數據
2. 預訓練：這些模型是在海量的網絡文本數據上進行預訓練，學會預測句子中缺少的單詞或生成合理的段落內容
3. 多用途：模型不僅可以用於對話，還可以用於翻譯、總結、文本生成、情感分析等許多不同的語言任務
4. 上下文理解：擅長處理長篇文章，並且能夠根據上下文理解意思，回答具體問題，甚至模擬某些專業領域的知識
  
大型語言模型是一種強大的自然語言處理技術，其能夠在許多不同的領域中應用，例如：
- 聊天機器人：像ChatGPT這樣的系統可以與人進行對話，解答問題或提供建議
- 內容生成：自動生成新聞文章、博客、故事等文本內容
- 語音助手：Siri、Google Assistant等助手使用類似的語言模型來理解用戶的需求並回應
- 翻譯工具：像Google翻譯這樣的工具使用大型語言模型來準確地翻譯語言

## What did I do
而相對的大型語言因為在大量文本數據上進行預訓練，因此需要花費較多的訓練時間和消耗較多的電腦效能，所以我將進行模型參數的調整以及整合，進而提升模型整體的品質，其衡量的指標是使用 loss/perplexity。
我在 Causal Language modeling 和 Masked language modeling 兩個模型中皆調整了參數epoch和學習率以及batch的大小，最後成功提升了模型的效能和品質，將低了loss !!

Causal Language modeling：
- [x] Training Loss 從2.555200降至2.403300
- [x] Validation Loss 從2.724581降至2.633503
- [x] Perplexity 從15.25降至13.92

Masked language modeling:
- [x] Training Loss 亦降至1.629200
- [x] Validation Loss 亦降至1.580378
